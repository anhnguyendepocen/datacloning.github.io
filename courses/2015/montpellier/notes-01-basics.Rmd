---
title: "Maximum Likelihood Estimation"
author: "Peter Solymos and Subhash Lele"
date: "August 1, 2015 -- Montpellier, France -- ICCB/ECCB Congress"
output: pdf_document
layout: course
course:
  year: 2015
  location: Montpellier
  title: "Hierarchical Models Made Easy &mdash; August 1, 2015 &mdash; Montpellier, France &mdash; ICCB/ECCB Congress"
  lecture: Basics
  next: notes-02-dc
---

### Introduction

Science, as we envision it, is an interplay between inductive and deductive processes. Francis Bacon, the father of what is known as the scientific method, emphasizes the roles of observations, alternative explanations and tests to choose among various explanations. Bacon saw science as inductive process, moving from the particular to the general (Susser, 1986). 

Popper (1959) proposed the doctrine of falsification, which defines what is acceptable as a scientific hypothesis: if a statement cannot be falsified, then it is *not* a scientific hypothesis. This is intrinsically a deductive process. What is common to these different views is that theories need to be probed to assess their correctness. Observations play an important role in such probing. 

In most scientific situations, we are interested in understanding the natural processes that have given rise to the observations. Such understanding generally leads to prediction and possibly control of the processes. Traditionally, we formulate our understanding of the processes in terms of mathematical models. These mathematical models may be deterministic or may be stochastic.

It is widely accepted, at least by the statisticians, that stochastic models represent nature more effectively than pure deterministic models. Aside from the natural stochasticity in the system, the observations themselves might have measurement error making it necessary to consider stochastic models to model observations. 

One of the consequences of stochastic models is that Popper's theory of falsification does not strictly apply. No data are strictly inconsistent with a stochastic model, except in artificial situations or trivially improper models. Thus, one can only say that the observed data are more likely under one model than the other; or that the strength of evidence for one hypothesis is larger than for an alternative. We cannot outright accept or reject a hypothesis. 

Given a set of stochastic models (or, equivalently a set of alternative descriptions of the underlying process), the goal of statistical inference is to choose the model that is best supported by the data. Thus, statistical inference is both deductive (it makes some predictions) and inductive (data determines which model is best supported). An important feature that we demand from all our statistical inference procedures is that with infinite amount of information, the probability of choosing the correct model converges to one (Royall, 2000). 

Another important feature of statistical inference is that it is uncertain. We want to know whether or not our inferential statement, namely the choice of the model, is trustworthy. Quantifying the uncertainty in the inferential statements is a critical issue and has led to different statistical philosophies of inference (e.g. Barnett, Royall, Taper and Lele), in particular the frequentist philosophy and the Bayesian philosophy. Just as numbers without understanding the units are meaningless, statistical inferential statements without proper understanding of the uncertainty are meaningless. 

We will discuss the differences in the two approaches to quantify uncertainty in the statistical inference in detail in the context of a simple example later. For the interested researcher, there are several resources available that discuss these issues in depth. We particularly recommend the books by Richard Royall (Royall, 1997), Vic Barnett (Barnett, 2002?) and an edited volume by Mark Taper and Subhash Lele (Taper and Lele, 2004). 

We also do not intend to give a detailed tutorial on the basics of statistical inference. There are many standard reference books for such introduction. For a mathematical and theoretical introduction see Casella and Berger and for more elementary but useful introduction, see e.g. Bolker (2000?).

### A simple example

Let us start with a simple occupancy model. We will use this model to introduce various important concepts that will be used throughout the course. We will use it also to introduce some basic commands for analyzing data using the R package dclone. 

In conservation biology, one of the first things we want to do is monitor the current status of the population. This can be done in terms of simple presence-absence data answering the question: what is the proportion of occupied sites? If this proportion is high, it may imply that we should not worry too much about the species (if it is something we want to maintain) or may be we want to do some biological control (if it is an invasive species). A simple monitoring procedure would consist of the following steps:

1. Divide the study area into quadrats of equal area. Suppose there are $N$ such quadrats. 
2. Take a simple random sample of size $n$ from these. 
3. Visit these sites and find out if it is occupied by the species or not. 

### Assumptions

It is critical that we state the assumptions underlying the statistical model. 
In practice, however, we may or may not be able to know whether all the 
assumptions are fulfilled or not. 

1. Quadrats are identical to each other. 
2. Occupancy status of one quadrat does not depend on the status of other quadrats.

Mathematically we write this as follows:

$$Y_{i} \sim Binomial(1, p)$$  (this is also known as the Bernoulli distribution) are independent, identically distributed (*i.i.d.*) random variables. 

Observed data: $$ Y_{1}, Y_{2}, \ldots, Y_{n} $$

Unobserved data: $$ Y_{n+1}, Y_{n+2}, \ldots, Y_{N} $$

The probability mass function of the Bernoulli random variable is written as: 
$$P(Y=y) = p^y (1-p)^{1-y}$$, where $$p \in (0,1)$$ and $$y=0,1$$.

We can now write down the likelihood function. This is proportional to the probability of observing the data at hand: 

$$L(p; y_{1}, y_{2}, \ldots, y_{n}) = \prod_{i=1}^{n} p^{y_{i}} (1-p)^{1-y_{i}}$$

We take product because observations are assumed to be independent of each other. 

### Important properties of likelihood

* Likelihood is a function of the parameter. 
* Data are fixed. 
* Likelihood is _not_ a probability of the parameter taking a specific value. It represents the following quantity: If the parameter is $$p=p^{\ast}$$, then the probability of observing the data at hand is $$L(\tilde{p}; y_{1}, y_{2}, \ldots, y_{n}) = \prod_{i=1}^{n} \tilde{p}^{y_{i}} (1-\tilde{p})^{1-y_{i}}$$. We can vary the parameter value and get a function as represented below:

```r
## random numbers from Binomial distribution
## Binomial with size=1 is a Bernoulli distribution
## p value set as 0.3
set.seed(1234) # set random seed for reproducibility
(y <- rbinom(n = 1000, size = 1, p = 0.3))
y1 <- y[1:10] # take only the 1st 10 elements of y

## pt is our p value that we want the Likelihood to be calculated for
pt <- 0.3
## the Likelihood is based on the formula from above
(L <- prod(pt^y1 * (1 - pt)^(1-y1)))
## the following statement is equivalent to typing in the formula
## take advantage of bilt-in density functions
prod(dbinom(y1, size = 1, prob = pt))

## now pt is a vector between 0 and 1
pt <- seq(0, 1, by = 0.01)
## use the sapply function to calculate the likelihood
## using one element of the vector at a time (argument z becomes prob=z)
## by fixing the data y1
L <- sapply(pt, function(z) prod(dbinom(y1, size = 1, prob = z)))
```

```r
op <- par(las=1) # always horizontal axis, store old settings in op
## color palettes for nice figures
flatly <- list(
    "red"="#c7254e",
    "palered"="#f9f2f4",
    "primary"="#2c3e50",
    "success"="#18bc9c",
    "info"="#3498db",
    "warning"="#f39c12",
    "danger"="#e74c3c",
    "pre_col"="#7b8a8b",
    "pre_bg"="#ecf0f1",
    "pre_border"= "#cccccc")
dcpal_reds <- colorRampPalette(c("#f9f2f4", "#c7254e"))
dcpal_grbu <- colorRampPalette(c("#18bc9c", "#3498db"))
## now we plot the Likelihood function
plot(pt, L, type = "l", col = flatly$info,
    main = paste("n =", length(y1)))
abline(v = p, lwd = 2, col = flatly$red) # true value
abline(v = pt[which.max(L)], lwd = 2, col = flatly$success) # ML extimate
```

### As we change the data, the likelihood function changes

```r
## function f has a single argument, n: sample size
f <- function(n) {
    y <- rbinom(n = n, size = 1, p = 0.5)
    L <- sapply(pt, function(z) prod(dbinom(y, size = 1, prob = z)))
    L / max(L)
}
## create a blank plot
plot(0, type = "n", main = "n constant, y changes",
     ylim = c(0, 1), xlim = c(0, 1),
     xlab = "pt", ylab = "L / max(L)")
## we simulate an n=25 data set 100 times and 
## plot the scaled likelihood function [L / max(L)]
tmp <- replicate(100, 
    lines(pt+runif(1,-0.01,0.01), f(25), 
    col = flatly$info))
abline(v = p, lwd = 2, col = flatly$red)
```

### As we increase the sample size, the likelihood becomes concentrated around the true value. 

```r
## try different sample sizes, data is fixed
## so samples can be nested
nvals <- seq(100, length(y), by = round(length(y)/10))
## scaled likelihood function using different sample sizes
Lm <- sapply(nvals, 
    function(n) {
        L <- sapply(pt, function(z) 
            prod(dbinom(y[1:n], size = 1, prob = z)))
        L / max(L)
    })
## plot the results
matplot(pt, Lm, type = "l", 
    lty = 1, ylab = "L / max(L)", main = "n increases",
    col=dcpal_grbu(length(nvals)))
abline(v = p, lwd = 2, col = flatly$red)
```

* Likelihood value represents the support in the data for a particular parameter value. This is intrinsically a relative concept. How much more support do we have for this parameter value vs. another parameter value. 
* Likelihood ratio is a more fundamental concept than the likelihood function itself. Law of the likelihood (Hacking, Royall): 

We can now summarize the goals of statistical inference:

1. Given these data, what is the strength of evidence for one hypothesis over the other hypothesis?
2. Given these data, how do we change our beliefs?
3. Given these data, what decision do we make?

> ### Check out the Shiny app
>
> Play around with the app to see how
> sample size affects the likelihood function.

### The maximum likelihood estimator

Which parameter value has the largest support in the data? 

We can use numerical optimization to get the value of a parameter
where the likelihood function is maximal.
Such a parameter value is called an (point) estimate,
while the function we are using to do the estimation (in this case th
likelihood function, but there might be other functions, too) is
called an estimator.

In numerical optimization, we often find the minimum of the
negative of a function, instead of finding the maximum.
Also, we use the log likelihood, because the product
becomes a sum on the log scale. This is much easier to compute.
That is why we ofteh find that programs define the negative
log likelihood function as we do below.

```r
## this functions simulates observations
sim_fun <- function(n, p) {
    rbinom(n = n, size = 1, p = p)
}
## this function returns the negative log likelihood value
nll_fun <- function(p, y) {
    -sum(dbinom(y, size = 1, prob = p, log = TRUE))
}
```

We use $n=100$ and $p=0.5$ for simulating the observation vector $y$.
Then use the one dimensional optimization function, `optimize`.
(For multivariate optimization problems, see the `optim` function.)

What is different between using optimization vs. manually setting up a set of values is that optimization starts with a sparse grid first to see
what region of the parameter space is of interest. In this region then 
the search for the minimum (or maximum) is continued with more intensity,
i.e. until the difference in subsequent candidate estimates reaches a pre defines
tolerance threshold (`tol` argument in `optimize`).

```r
n <- 100
p <- 0.5
y <- sim_fun(n, p)
optimize(nll_fun, interval = c(0, 1), y = y)
```

Once we can write down the likelihood, we can in principle
write a program to calculate the value of the (negative log) likelihood
function given some parameter value and the data.

### The sampling distribution of the estimates

Let us revisit now what happened when we kept the sample size fixed but changed
the data. In this case, we get different parameter estimates (MLEs) for different
data sets. A natural question to ask would be: How much would the answers vary if we have different samples? 

In the following program we pre-allocate a vector of length $B$, we simulate
the data $B$ times and store the corresponding MLEs in the object `res`:

```r
B <- 1000
res <- numeric(B)
for (i in 1:B) {
    y <- sim_fun(n, p)
    res[i] <- optimize(nll_fun, interval = c(0, 1), y = y)$minimum
}
```

Some summary statistics reveal interesting things: the $B$ estimates now
have a *sampling distribution* that can be characterized by its
mean and various percentiles:

```r
summary(res)
quantile(res, c(0.025, 0.975))
```

### Bias and consistency

The bias is defined as the deviation between the
estimate and the true parameter values.
When the bias converges to 0 whith increasing sample size,
we say that an estimator is consistent:

```r
mean(res - p)
```

Precision is the corresponding feature of an estimate.

### Confidence interval and efficiency

The 2.5% and 97.5% percentiles of the sampling distribution correspond to the
95% analytical confidence intervals around the true parameter value.

```r
level <- 0.95
a <- (1 - level) / 2
a <- c(a, 1 - a)
(ci0 <- quantile(res, a))
```

An estimator is called efficient when the variation in the sampling
distribution and the confidence interval gets smaller with increasing
sample size. Accuracy is the corresponding feature of an estimate.
When the percentiles of the sampling distribution 
are close to the corresponding analytical confidence intervals, we
say that the estimator has nomila coverage.

The following plot shows tha sampling distribution of the
estimates, the true parameter value, the mean of the estimates,
the analytical confidence intervals and the quantiles of the
sampling distribution. The values overlap perfectly, that is why the red
lines are not visible:

```r
hist(res, col = flatly$palered, border = flatly$pre_border,
    xlim = c(0, 1))
rug(res+runif(B, -0.01, 0.01), col = flatly$info, lwd = 2)
## sampling distribution based sumarry statistics
abline(v = mean(res), lwd = 2, col = flatly$success)
abline(v = quantile(res, c(0.025, 0.975)), 
    lwd = 2, col = flatly$success, lty = 2)
```

### Estimated confidence intervals

Of course, in real life, we do not have the luxury of conducting such repeated experiments. So what good are these ideas? 

One vay to quentify the uncertainty in the estimate is to use
asymptotic conficence intervals as we saw above. We called it analytical
because for this particular model we could calculate it analytically.
This, however, mignt not be the case in all situation. One can estimate
the asymptotic standard error and conficence interval of an estimate:

```r
## our data
y <- sim_fun(n, p)
## MLE
(est <- optimize(nll_fun, interval = c(0, 1), y = y)$minimum)
(ci1 <- qnorm(a, mean = est,
    sd = sqrt(est * (1-est) / n)))
```

We have the MLE. The MLE is kind of close to the true parameter value. So suppose we pretend as if the MLE is the true parameter value, we can get the sampling distribution and the confidence interval. This is the idea behind the parametric bootstrap confidence intervals. 

```r
B <- 1000
pbres <- numeric(B)
for (i in 1:B) {
    yb <- sim_fun(n, est) # treat est as 'true' value and estimate
    pbres[i] <- optimize(nll_fun, interval = c(0, 1), y = yb)$minimum
}
(ci2 <- quantile(pbres, a))
```

The non-parametric bootstrap is based on a similar principle,
but instead of simulating data sets under our initial estimate,
we mimic the experiment by resampling the original data set
with replacement $B$ times:

```r
## we use the same settings and data as for non-parametric bootstrap
npbres <- numeric(B)
for (i in 1:B) {
    yb <- sample(y, replace = TRUE)
    npbres[i] <- optimize(nll_fun, interval = c(0, 1), y = yb)$minimum
}
(ci3 <- quantile(npbres, a))
```

Let us compare the true CI with the estimated CIs:

```r
cbind(true = ci0, asy = ci1, pb = ci2, npb = ci3)
```

### Summary

This kind of analysis is called the frequentist analysis. We are studying the properties of the inferential statement under the hypothetical replication of the experiment. This analysis tells us about the reliability of the procedure. 

The implicit logic is that if the procedure is reliable, we could rely on the inferential statements obtained from only one data set. We choose a procedure that is most reliable. 

This is similar to relying more on the blood pressure results from a machine that has small measurement error instead of one with large measurement error. 


### Bayesian analysis 

One major criticism of the Frequetist approach is that we do not repeat the experiment. What we want to know is: What do the data at hand tell us? 
Bayesian approach does not quite answer that question but answers a different question: Given these data, how do I change my beliefs? 

Our goal is to infer about the true parameter value (the true occupancy proportion). 
Prior distribution: This quantifies in probabilistic terms our personal beliefs about the true occupancy rate. 

We may believe that it is most likely to be 0.7. Then we consider distribution with mode at 0.7. We cannot determine the entire distribution from such information. But that is what a Bayesian inference demands. It is a very difficult task but is a necessary task if you want to use the Bayesian approach. 

Posterior distribution: This quantifies the beliefs as modified by the data. The mathematical formulation is as follows:

$$\pi(\theta \mid y) = \frac{L(\theta;y) \pi(\theta)}{ \int L(\theta;y) \pi(\theta) d\theta }$$

$\pi(\theta)$ is the prior distribution.


> ### Shiny apps to illustrate the following features
> 
> * Beta priors (including Uniform and Jeffrey's prior):
>   - Different priors, same data lead to different posteriors.
>   - Same prior, different data lead to different posteriors. 
>   - As the sample size increases, the posterior is invariant to the prior 
>     (eventually degenerate at the true value).
> * Normal prior:
>   - check the effects of probability vs. log scale on the prior and posterior.
> * Bimodal Normal mixture prior:
>   - check how a bimodal prior changes the posterior.

Here the code for the Beta priors:

```r
## Bayesian posterior calculations
y0 <- sim_fun(100, p)
y <- y0[1:10]
pval <- seq(0.001, 0.999, by = 0.0005)
## beta(0.5,0.5) is the Jeffrey's prior
## beta(1,1) is Uniform prior
fPri <- function(p, shape1=0.5, shape2=0.5)
    dbeta(p, shape1, shape2)
fLik <- function(p, y)
    prod(dbinom(y, size = 1, prob = p))
Lik <- sapply(pval, fLik, y=y[1:input$n])
Pri <- sapply(pval, fPri, 0.5, 0.5) # change the prior here
Pos <- Lik * Pri
M <- cbind(Pri=Pri/max(Pri),
    Lik=Lik/max(Lik),
    Pos=Pos/max(Pos))
Col <- c("#cccccc", "#3498db", "#f39c12")
matplot(pval, M, type = "l", 
    col=Col, lwd=2, lty=1,
    ylab = "Density", 
    xlab=ifelse(input$scale == "logit", "logit(p)","p"),
    sub=paste0("Mean = ", round(mean(y[1:input$n]), 2), " (", 
        sum(1-y[1:input$n]), " 0s & ", sum(y[1:input$n]), " 1s)"),
    main = paste("Posterior mean =", round(sum(pval * M[,"Pos"]), 2)))
abline(v = input$p, lwd = 2, col = "#c7254e")
abline(v = sum(pval * Pos/sum(Pos)), lwd = 2, col = "#18bc9c")
legend("topleft",lty=1, lwd=2, col=Col, bty="n",
   legend=c("Prior","Likelihood","Posterior"))
```


### Non-informative priors (Objective Bayesian analysis)

It is clear that priors have an effect on the Bayesian inference. And, they should have an effect. However, this subjectivity is bothersome to many scientists. There is an approach that is euphemistically called an objective Bayesian approach. In this approach, we try to come up with priors that have least influence on the inference (e.g. Bernardo, 1980). There is no unique definition of what we mean by non-informative priors. Various priors have been suggested in the literature. The most commonly used non-informative priors are: Uniform priors and the large variance priors. Other priors are nearly impossible to specify for the complex models that ecologists are faced with. 

Do these priors affect the inference? 

The answer is that ''they most certainly do''. There is nothing wrong with the priors affecting the inference as long as the researchers can justify their priors. 

> Uniform prior and its effect: prob scale
>
> Log-normal prior and its effect: normal prior on logit scale
>
> Jeffrey's prior and its effect: not affected
>
> for same data you get different answers

Fundamentally there is no such thing as 'objective' Bayesian inference. Those who want to use Bayesian inference should simply accept that the inferences are affected by the choice of the priors. 



### Data cloning: How to trick Bayesians into giving Frequentist answers?

> Difference between Frequentist and Bayesian inferential statements:
>
> Summarize the philosophical differences between the inferential statements made 
> by a Frequentist and a Bayesian in the context of occupancy model. 

Add here: not increasing n but copy the data, see what happens (should be the same as more n)
--> another app for DC (show MLE as well)


### Data cloning for Frequentist predictive distribution:

MORAL: If the sample size is large, the numerical differences between the Frequentist and Bayesian answers vanish. However, their interpretation is different. 

### What have we learnt?

* Data generation
* Likelihood (Analytical, Graphical and also in JAGS)
* Frequentist inference 
* Bayesian inference 
* Data cloning to get MLE and confidence intervals 

***

### Continuous observations

> Peter: 
> Can you come up with an ecological situation where the response is continuous? 
> And, we want to do prediction of the unobserved data? 

Let us look at the likelihood function and all the other things we did so far when the observations are continuous. We will have exactly the same programs but with Normal distribution substituted for the Bernoulli distribution. 

Prediction of the unobserved data:

This is where there is substantial difference between the frequentist method and Bayesian method.

> #### R commands:
> True predictive distribution: This will be under the true parameter values. 

Can we get a reasonable approximation of this distribution? 

1. Estimated predictive distribution: This will change from one sample to the other. 
2. Frequentist predictive distribution: 
3. Bayesian predictive distribution: 

What do we mean by: Is the prediction interval correct? 

Coverage probability: 
Estimated predictive distribution has smaller than nominal coverage.
Frequentist and Bayesian predictive distributions are wider and hence tend to have better coverage. 
Obviously, the Bayesian predictive coverage depends on the prior distribution. For some priors, one gets really good coverage and some can have really bad coverage. 
Frequentist predictive coverage is also not guaranteed to have good coverage. 
As the sample size increases, both Bayesian and Frequentist predictive distributions converge to the true predictive distribution. 


> (This will illustrate how to extend the standard model file to accommodate data 
> cloning dclone with dcdim command as well as init values). We also remind them 
> to check the results with different priors to make sure the MCMC is not stuck.

### DAGs

```r
library(diagram)

## Bernoulli
par(mar = c(1, 1, 1, 1))
openplotmat(main = "Bernoulli")
pos <- coordinates(3)
straightarrow(from = pos[2, ], to = pos[1, ])
straightarrow(from = pos[3, ], to = pos[2, ])
textrect(pos[1,], 0.05, 0.05, lab="Y", shadow.size=0, box.col="#ecf0f1")
textellipse(pos[2,], 0.05, 0.05, lab="p", shadow.size=0)
textdiamond(pos[3,], 0.05, 0.05, lab=expression(pi[p]), shadow.size=0)

## Bernoulli-Bernoulli
par(mar = c(1, 1, 1, 1))
openplotmat(main = "Bernoulli-Bernoulli")
pos <- coordinates(c(3,3))
straightarrow(from = pos[2, ], to = pos[1, ])
straightarrow(from = pos[3, ], to = pos[2, ])
straightarrow(from = pos[1, ], to = pos[4, ])
straightarrow(from = pos[6, ], to = pos[5, ])
straightarrow(from = pos[5, ], to = pos[4, ])
textellipse(pos[1,], 0.05, 0.05, lab="Y", shadow.size=0)
textellipse(pos[2,], 0.05, 0.05, lab=expression(phi), shadow.size=0)
textdiamond(pos[3,], 0.05, 0.05, lab=expression(pi[phi]), shadow.size=0)
textrect(pos[4,], 0.05, 0.05, lab="W", shadow.size=0, box.col="#ecf0f1")
textellipse(pos[5,], 0.05, 0.05, lab="p", shadow.size=0)
textdiamond(pos[6,], 0.05, 0.05, lab=expression(pi[p]), shadow.size=0)

## Binomial-Poisson
par(mar = c(1, 1, 1, 1))
openplotmat(main = "Binomial-Poisson")
pos <- coordinates(c(3,3))
straightarrow(from = pos[2, ], to = pos[1, ])
straightarrow(from = pos[3, ], to = pos[2, ])
straightarrow(from = pos[1, ], to = pos[4, ])
straightarrow(from = pos[6, ], to = pos[5, ])
straightarrow(from = pos[5, ], to = pos[4, ])
textellipse(pos[1,], 0.05, 0.05, lab="N", shadow.size=0)
textellipse(pos[2,], 0.05, 0.05, lab=expression(lambda), shadow.size=0)
textdiamond(pos[3,], 0.05, 0.05, lab=expression(pi[lambda]), shadow.size=0)
textrect(pos[4,], 0.05, 0.05, lab="Y", shadow.size=0, box.col="#ecf0f1")
textellipse(pos[5,], 0.05, 0.05, lab="p", shadow.size=0)
textdiamond(pos[6,], 0.05, 0.05, lab=expression(pi[p]), shadow.size=0)

## Normal-Normal

## Binomial-Lognormal

## Poisson-Lognormal

## Gompertz/Ricker

```


***

## Sandbox

$$(y \mid X = x) \sim h(y; X = x, \theta_{1})$$

$$X \sim g(x; \theta_{2})$$

$$\theta = (\theta_{1}, \theta_{2})$$

$$L(\theta;y) = \int h(y \mid x; \theta_{1}) g(x; \theta_{2}) dx$$

Posterior distribution:

$$\pi(\theta \mid y) = \frac{L(\theta;y) \pi(\theta)}{ \int L(\theta;y) \pi(\theta) d\theta }$$

$\pi(\theta)$ is the prior distribution.

Normal&ndash;Normal model:

$$Y_{ij} \mid \mu_{ij} \sim Normal(\mu_{ij}, \sigma^2)$$

$$i=1,\ldots,n; j=1,\ldots,m_{n}$$

$$\mu_{ij} = X_{ij}^{T}\theta + \epsilon_{i}$$

$$\epsilon_{i} \sim Normal(0, \tau^2)$$

```r
"model {
    for (ij in 1:nm) {                #### <- likelihood
        Y[ij] ~ dnorm(mu[ij], 1/sigma^2)
        mu[ij] <- inprod(X[ij,], theta) + e[gr[ij]]
    }
    for (i in 1:n) {
        e[i] ~ dnorm(0, 1/tau^2)
    }
    for (k in 1:np) {                 #### <- priors
        theta[k] ~ dnorm(0, 0.001)
    }
    sigma ~ dlnorm(0, 0.001)
    tau ~ dlnorm(0, 0.001)
}"
```

```r
library(rjags) 
library(dclone)
set.seed(1234)
theta <- c(1, -1)
sigma <- 0.6
tau <- 0.3
n <- 50 # number of clusters
m <- 10 # number of repeats within each cluster
nm <- n * m # total number of observations
gr <- rep(1:n, each=m) # group membership defining clusters
x <- rnorm(nm) # covariate
X <- model.matrix(~x) # design matrix
e <- rnorm(n, 0, tau) # random effect
mu <- drop(X %*% theta) + e[gr] # mean
Y <- rnorm(nm, mu, sigma) # outcome

model <- custommodel(c("model {
    for (ij in 1:nm) {                #### <- likelihood
        Y[ij] ~ dnorm(mu[ij], 1/sigma^2)
        mu[ij] <- inprod(X[ij,], theta) + e[gr[ij]]
    }
    for (i in 1:n) {
        e[i] ~ dnorm(0, 1/tau^2)
    }
    for (k in 1:np) {                 #### <- priors
        theta[k] ~ dnorm(0, 0.001)
    }
    sigma ~ dlnorm(0, 0.001)
    tau ~ dlnorm(0, 0.001)
}"))

dat <- list(Y=Y, X=X, nm=nm, n=n, np=ncol(X), gr=gr)
str(dat)

m <- jags.fit(dat, c("theta", "sigma", "tau"), model, n.update=2000) 
summary(m)
pairs(m)
```

DC:

$$y^{(K)} = (y,\ldots,y)$$

$$L(\theta;y^{K}) = L(\theta;y)^{K}$$

$$\pi_{K}(\theta \mid y) = \frac{[L(\theta;y)]^{K} \pi(\theta)}{ \int [L(\theta;y)]^{K} \pi(\theta) d\theta }$$

$$\pi_{K}(\theta \mid y) \sim MVN(\hat{\theta}, \frac{1}{K} I^{-1}(\hat{\theta}))$$

```r
str(dclone(dat, n.clones=2, unchanged="np", multiply=c("nm", "n")))
mk <- dc.fit(dat, c("theta", "sigma", "tau"), model, n.update=2000,
    n.clones=c(1,2,4,8), unchanged="np", multiply=c("nm", "n"))
op <- par(mar=0.6*c(5, 4, 4, 2) + 0.1, cex=0.5)
plot(dctable(mk), position=c(-100, -100))
par(op)
```

## PDF transformation trick with KDE

## analytic results for Logit-Normal

The problem is this: X is a random variable say Normal(2,4). I can plot histogram of 1000 draws, and have the PDF

```r
hist(rnorm(1000, 2, 2), freq=FALSE)
curve(dnorm(x, 2, 2), add=TRUE, col=2)
```

Now what is the PDF of inverse_logit(X)?

```r
dlogitnorm <- function(x, mean = 0, sd = 1, log = FALSE) {
    if (log) {
        log(1/(x*(1-x))) + dnorm(qlogis(x), mean, sd, TRUE)
    } else {
        (1/(x*(1-x))) * dnorm(qlogis(x), mean, sd)
    }
}
hist(plogis(rnorm(1000, 2, 2)), freq=FALSE)
curve(dlogitnorm(x, 2, 2), from=0, to=1, add=TRUE, col=2)
```

Let's approximate the result using KDE

```r
x <- rnorm(10000, 2, 2)
dr <- density(x)
hist(x, freq=FALSE)
curve(dnorm(x, 2, 2), add=TRUE, col=2)
lines(dr, col=4)
```

On the probability scale we get:

```r
dp <- dr
dp$dx <- diff(dp$x)
dp$dx <- c(dp$dx[1], dp$dx)
dp$gx <- plogis(dp$x)
dp$dgx <- diff(dp$gx)
dp$dgx <- c(dp$dgx[1], dp$dgx)
dp$gy <- dp$dx * dp$y / dp$dgx

hist(plogis(x), freq=FALSE)
curve(dlogitnorm(x, 2, 2), from=0, to=1, add=TRUE, col=2)
lines(dp$gx, dp$gy, col=4)
```


## Beta

The problem is this: X is a random variable say Beta(0.5,1). I can plot histogram of 1000 draws, and have the PDF

```r
hist(rbeta(1000, 0.5, 1), freq=FALSE)
curve(dbeta(x, 0.5, 1), add=TRUE, col=2)
```

Now what is the PDF of logit(X)?

```r
x <- rbeta(10^6, 0.5, 1)
hist(qlogis(x), freq=FALSE)
lines(density(qlogis(x)), col=4)
```

